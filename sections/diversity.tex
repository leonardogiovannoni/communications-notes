\section*{Transmissions over Fading Channels}

\begin{itemize}
    \item Signal fading is the main problem in wireless communications.
    \item OFDM is a technique designed to combat the destructive effects of multipath fading.
    \item Slow flat Rayleigh fading is still a big problem.
    \item One of the most effective resources against the effects of channel fading is \textbf{diversity}.
\end{itemize}

\section*{Diversity in Wireless Communications}

\begin{itemize}
    \item \textbf{Diversity} refers to the possibility of improving the reliability of a message by transmitting it over two or more communication channels with different characteristics.
    \item Diversity is a common technique for combatting fading and co-channel interference and avoiding error bursts.
\end{itemize}


\section*{Diversity in Wireless Communication Systems}

\subsection*{Time Diversity:}
Time diversity relates to the \textit{coherence time} of the channel and includes techniques such as transmission over multiple time slots by channel coding plus interleaving. It is particularly effective over very slow fading channels.

\subsection*{Frequency Diversity:}
Frequency diversity relates to the \textit{coherence bandwidth} of the channel. It involves transmission over multiple frequency bands and offers advantages over very flat fading channels.

\subsection*{Spatial Diversity:}
Spatial diversity is associated with the \textit{coherence distance} and involves the transmission and reception employing multiple antennas.

\section*{Time Diversity: Interleaving and Coding}
\begin{itemize}
    \item Initially proposed by Claude Shannon in 1948, channel coding introduces redundancy to detect errors at the receiver and improve the bit error probability.
    \item Redundancy is quantified by the code rate $R = \frac{k}{n}$, where $k$ is the number of input bits, and $n$ is the number of output bits from the encoder.
    \item While initially studied for AWGN channels, channel codes are applicable to fading channels to enhance reliability.
\end{itemize}



\section*{Error Detection Coding}

A straightforward technique in error detection is the use of parity bits. Parity bits are additional bits at the end of a data word used for error detection. They provide a simple way to check the integrity of the data. 

\subsection*{Parity Check Code}
A parity check code is an error detection code where the code rate \( R \) is determined by the ratio of information bits to total bits, including the parity bit(s). 

For example, consider a parity check code with rate \( R = \frac{7}{8} \):
\begin{itemize}
    \item The word is composed of \( k = 7 \) bits.
    \item A single parity bit is added, making \( n = 8 \) bits in total.
    \item The generator matrix \( \mathbf{G} \) for such a code can be expressed as a \( 7 \times 8 \) matrix, where the additional column is used for the parity bit, ensuring that the word always contains an even number of '1's.
    \item The encoded vector \( \mathbf{d} \) is then given by \( \mathbf{d} = \mathbf{uG} \), where \( \mathbf{u} \) is the vector of information bits.
\end{itemize}

The encoded word is an 8-bit vector with the property that it contains an even number of '1's if the original 7-bit word contained an odd number of '1's, and vice versa.

At the decoder side:
\begin{itemize}
    \item The receiver computes the parity check by examining the number of '1's in the received 8-bit word.
    \item If the number of '1's is even, the word is considered error-free.
    \item If the number of '1's is odd, this indicates that an error has occurred, assuming only single-bit errors are possible.
\end{itemize}

Not all errors can be detected with this method, particularly if an even number of bits are in error, as this will not affect the parity check condition.


\section*{Data Retransmission and Channel Capacity}

\subsection*{Data Retransmission}
\begin{itemize}
    \item Automatic Repeat Request (ARQ) is a protocol for error control in data transmission where the receiver sends back an acknowledgment (ACK) for correctly received packets and a negative acknowledgment (NACK) for faulty ones.
    \item When a NACK is received, the transmitter resends the data packet. This method exploits the \textit{time diversity} of the channel by retransmitting the data after a time interval longer than the channel coherence time \( T_c \).
    \item Advanced receivers can combine multiple received messages to improve the chances of successful reception, known as \textit{diversity combining}.
\end{itemize}

\subsection*{Error Correction Coding and Channel Capacity}
\begin{itemize}
    \item Error correction coding is utilized to correct errors introduced by the channel during transmission.
    \item Shannon's theorem establishes that for a communication channel of bandwidth \( B \), the channel capacity \( C \) is the upper bound on the rate at which information can be reliably transmitted over the channel and is given by:
    \[ C = B \log_2 (1 + \text{SNR}) \text{ bits/s}, \]
    where SNR is the Signal-to-Noise Ratio.
    \item If the transmission rate \( R \) is less than the channel capacity \( C \), it is theoretically possible to design an error correction code that makes the probability of error as low as desired, but not zero.
    \item On the contrary, if \( R > C \), it is impossible to guarantee an arbitrarily low error probability.
\end{itemize}

\subsection*{Practical Example}
\begin{itemize}
    \item In a communication system with bandwidth \( B = 1.8 \) MHz and using 16-QAM modulation, if the symbol rate \( R_s = \frac{1}{T} \) is such that \( R_s \cdot B \log_2(1 + \text{SNR}) \) does not exceed \( 40 \) Mbits/s, the transmission is within the channel capacity and is thus feasible.
    \item Otherwise, if the product of symbol rate and channel capacity is exceeded, the transmission would exceed the channel's capability to convey information without errors, leading to an increased error rate.
\end{itemize}






\section*{Block Codes: Repetition Code and Decoding}

\subsection*{Repetition Code}
\begin{itemize}
    \item The simplest form of block codes is the repetition code.
    \item For \( k = 1 \) and \( n = 3 \), the \( 3 \)-repetition code is used, which can be considered as a \( (1,3) \) code.
    \item The generator matrix for the code is \( G = \begin{bmatrix} 1 & 1 & 1 \end{bmatrix} \).
    \item Encoding of a bit \( u \) is done as \( d = uG \), resulting in \( d \) being either \( [0 \, 0 \, 0] \) or \( [1 \, 1 \, 1] \) for \( u = 0 \) or \( u = 1 \), respectively.
\end{itemize}

\subsection*{Decoder Operation}
\begin{itemize}
    \item The decoder uses majority decision decoding.
    \item Given a received bit string \( \hat{d} \), the decoder computes \( u = \hat{d} G^T \) and makes a decision based on the majority of bits.
    \item The rate \( R \) of the repetition code is \( \frac{1}{3} \).
\end{itemize}

\subsection*{Hamming Distance and Error Detection}
\begin{itemize}
    \item The Hamming distance is utilized to measure the distance between two codewords.
    \item For error detection, \( \hat{d} \) is chosen such that the Hamming distance \( d(\hat{d},d) \) is minimized.
    \item The Hamming distance provides a measure of error correction capability and error detection.
\end{itemize}


\section*{Block Codes: Decoder}

\begin{itemize}
    \item \textbf{Error Events:} An error event in block codes occurs when noise causes the received vector \(\hat{x}\) to be closer to a codeword that is different from the transmitted one.
    \item \textbf{Robustness of Codes:} Codes that have a larger Hamming distance between words are more robust against noise and fading compared to codes with a smaller Hamming distance.
    \item \textbf{Hamming Distance:} The Hamming distance is a metric that increases with the length of the code, specifically the parameters \(k\) (number of information bits) and \(n\) (length of the codeword). Unfortunately, the complexity of the receiver that must decode these codes also grows with \(k\).
\end{itemize}

\section*{Implications on Decoder Complexity}

The robustness of error-correcting codes and the ability of a decoder to correct errors without requiring retransmission are essential for efficient communication systems. However, this error correction capability comes at the cost of increased receiver complexity. This complexity is influenced by factors such as the length of the codeword \(n\) and the number of information bits \(k\). As these parameters increase, the computational effort required to decode the received messages also increases, which can impact the processing speed and energy consumption of communication devices.


\section*{Convolutional Codes: Encoder}

\begin{itemize}
    \item The encoder of a convolutional code, denoted as \((n, k, L)\), operates as \(n\) parallel linear filters over the Galois field GF(2).
    \item The parameter \(L\) is the \emph{constraint length} of the encoder, determining how many previous \(k\)-bit input words affect the \(n\)-bit output.
    \item Unlike block codes, convolutional codes have \emph{memory}: each output bit is a function of the current and \(L-1\) previous input bits, introducing temporal correlation in the encoded sequence.
\end{itemize}

\section*{Code Generators for Convolutional Codes}

\begin{itemize}
    \item The impulse response of the \(n\) linear filters defines the convolutional code's generator vectors, each of length \(L\cdot k\).
    \item In GF(2), these impulse responses consist of binary sequences representing the filter taps.
    \item The output codeword bits are computed by convolving the input bits with these generator sequences.
\end{itemize}

\subsection*{Example: Convolutional Code Generation}
Given a single-bit input (\(k=1\)) and two output bits (\(n=2\)), with \(L=3\), the convolutional encoding can be represented as follows:
\begin{align*}
    d^{(i)}_j &= \sum_{l=0}^{2} g_j(l)\cdot u^{(i-l)} \quad \text{for } j = 1,2 \\
    &= g_j(0)\cdot u^{(i)} + g_j(1)\cdot u^{(i-1)} + g_j(2)\cdot u^{(i-2)}
\end{align*}
where \(g_j(l)\) are the elements of the generator vectors, and \(u^{(i)}\) is the \(i\)-th input bit.

\section*{Matrix Representation}
The systematic form of a convolutional encoder can also be represented using a generator matrix \(G\), though the inherent memory aspect of convolutional codes makes this representation more complex than for block codes.

\[
G = 
\begin{bmatrix}
    g_{1,0} & g_{1,1} & \dots & g_{1,L-1} \\
    g_{2,0} & g_{2,1} & \dots & g_{2,L-1} \\
    \vdots  & \vdots  & \ddots & \vdots    \\
    g_{n,0} & g_{n,1} & \dots & g_{n,L-1} \\
\end{bmatrix}
\]


\section*{The Convolutional Code (2,1,3)}

Let's delve deeper into the specifics of the (2,1,3) convolutional code.

\subsection*{Encoding Process}
The generator polynomials for this code are $g_1 = [1\ 1\ 1]$ and $g_2 = [1\ 0\ 1]$, with the code rate $R = \frac{1}{2}$. The codeword bits are calculated using:
\begin{align*}
    d_1^{(i)} &= u^{(i)} + u^{(i-1)} + u^{(i-2)} \\
    d_2^{(i)} &= u^{(i)} + u^{(i-2)}
\end{align*}
where $u^{(i)}$ represents the input bit at time index $i$.

\subsection*{State Diagram Representation}
The encoder can also be represented by a state diagram, illustrating the finite state machine nature of the encoder:

In the diagram, each state transition corresponds to an input bit (above the line) and the resulting encoded output bits (below the line).

\subsection*{Encoding Memory}
It's crucial to note that the convolutional encoder has memory, which is reflected in the state transitions. The current state along with the input bit determines the next state and the output bits.








\section*{Convolutional Codes}

\subsection*{Trellis Diagram}
The trellis diagram for the (2,1,3) encoder is a graphical representation that shows all possible transitions between states at each time step based on the input bit. The red path indicates the sequence of states and outputs for a given input.
\subsection*{Decoder Functionality}
Convolutional code decoders process the received bit stream and attempt to reconstruct the transmitted information sequence. The decoder selects the path through the trellis diagram that best matches the received sequence, which is the path with the minimum Hamming distance to the received sequence.
\section*{Convolutional Codes: The Viterbi Algorithm}

\subsection*{Introduction}
\begin{itemize}
  \item The Viterbi algorithm, presented in 1967 by Andrew Viterbi, revolutionized the decoding of convolutional codes, reducing the complexity from exponential to linear in \( N \), where \( N \) is the length of the encoded sequence.
  \item It operates by evaluating the minimum Hamming distance of all possible paths through a trellis diagram representing the state transitions of the encoder.
\end{itemize}

\subsection*{Algorithm Overview}
The Viterbi algorithm simplifies the decoding process by:
\begin{enumerate}
  \item Identifying and expanding only the most likely paths at each step (also known as `survivor paths').
  \item Computing a `cumulated metric' for each path, which is a running sum of the `branch metrics', measures of how well the path matches the received sequence.
  \item Discarding less likely paths, thus reducing the need to compute the entire trellis, which is \( 2^{kN} \) for a block of \( N \) bits and \( k \) information bits.
  \item Selecting the path with the lowest cumulative metric at the end of the decoding process, representing the most likely transmitted sequence.
\end{enumerate}

\subsection*{Practical Impact}
\begin{itemize}
  \item The Viterbi algorithm is widely used in various communication systems, including those that require real-time decoding, due to its efficiency and performance.
\end{itemize}



\section*{Interleaving in Communication Systems}

Convolutional codes are well-suited for memoryless channels with random error events. They are most effective when errors are uniformly distributed and uncorrelated. However, fading channels often cause errors that are bursty in nature, meaning that errors tend to be correlated over time. 

Interleaving is a technique used to combat the effect of burst errors. It rearranges the order of the transmitted symbols according to a certain pattern. When the sequence passes through a channel that causes burst errors, these errors will be spread out in the deinterleaved sequence at the receiver. This makes the channel seem more like a memoryless channel, which helps standard error-correcting codes to perform error correction more efficiently by decorrelating error events.

An example of block interleaving is shown below, where the sequence of symbols is rearranged before transmission:

\begin{verbatim}
Original sequence: A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P
Interleaved sequence: A, E, I, M, B, F, J, N, C, G, K, O, D, H, L, P
\end{verbatim}

\section*{Block Interleaver Example}
Consider an interleaver with the following input and output:
\begin{align*}
\text{Input:} & \quad (A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P) \\
\text{Interleaved Output:} & \quad (A,E,I,M,B,F,J,N,C,G,K,O,D,H,L,P)
\end{align*}

This interleaving strategy ensures that errors affecting consecutive symbols in the original sequence will be spread out in the transmitted sequence, thus reducing the probability of burst errors.



\section*{Interleaving}
Interleaving is a technique applied in communication systems for error correction, particularly effective against burst errors. It works by spreading the coded symbols across the time or frequency domain, which makes bursty error patterns appear random, thereby improving the performance of convolutional codes.

\textbf{Interleaving Considerations:}
\begin{itemize}
    \item It introduces latency in the transmission process.
    \item The depth of the interleaver (\( K \)) is proportional to the de-correlation of errors but also to the introduced latency.
    \item Types of interleaving include block and convolutional (or cross) interleaving.
\end{itemize}

\section*{Turbo Codes and LDPC}
Advanced coding techniques, such as Turbo codes and Low-Density Parity-Check (LDPC) codes, mark significant milestones in channel coding theory. These coding strategies approach the Shannon limit, which is the theoretical maximum data rate of a noisy channel:

\[ C = B \log_2 (1 + \text{SNR}) \text{ bits/s} \]

\begin{itemize}
    \item Turbo codes, introduced in 1993, utilize a feedback loop in the encoding process.
    \item LDPC codes, introduced in 1999, use a sparse parity-check matrix.
    \item Both coding techniques offer near-Shannon limit performance.
\end{itemize}


\section*{Turbo Codes}

Turbo codes are a class of high-performance error correction codes that employ a parallel concatenation of two or more convolutional codes, separated by an interleaver.

\subsection*{Encoding Process}
\begin{enumerate}
    \item Data bits are input to the first encoder to produce a set of parity bits.
    \item The data bits are then interleaved, effectively shuffling the order of the bits, to spread out bursts of errors.
    \item The interleaved bits are input to a second encoder to produce another set of parity bits.
\end{enumerate}

\subsection*{Decoding Process}
Turbo decoding employs an iterative algorithm using two decoders, which pass soft information back and forth between each other.

\begin{enumerate}
    \item The first decoder processes the received data and produces extrinsic information.
    \item This extrinsic information is then de-interleaved and passed as a priori information to the second decoder.
    \item The second decoder refines the information and passes it back to the first decoder after interleaving.
    \item This iterative process continues, improving the estimate of the transmitted data with each iteration.
\end{enumerate}

\subsection*{Performance}
The performance of Turbo codes approaches the Shannon limit, making them highly effective for communication systems where bandwidth efficiency is critical.




\section*{Turbo Codes and Latency}

Turbo codes are a type of error-correcting code that use iterative decoding and interleaving to improve performance. However, this can introduce latency.

\subsection*{Latency in Turbo Codes}
Convolutional codes, and especially Turbo codes, face a trade-off between decoding performance and latency. The iterative process of Turbo decoding and the use of interleavers to randomize bit errors across the transmitted data block both contribute to this latency.

\subsection*{Managing Latency}
\begin{itemize}
    \item At any given Signal-to-Noise Ratio (SNR), system designers must balance the latency caused by the interleaver and the Quality of Service (QoS) required by the application.
    \item For real-time voice communication, which can tolerate a medium to high Bit Error Rate (BER), smaller block sizes (\( K \approx 300 \) bits) are preferred.
    \item For video playback, where a lower BER is essential, mid-range block sizes (\( K \approx 4000 \) bits) are typically used.
    \item For file transfers that can tolerate very low BERs, larger block sizes (\( K \approx 16000 \) bits) are chosen to optimize throughput despite the increased latency.
\end{itemize}






\section*{Spatial Diversity}

Spatial diversity is a form of diversity technique utilized in wireless communications to enhance signal robustness to fading. It involves the use of multiple antennas at the transmitter and/or receiver to create multiple independent channels for the same signal.

\subsection*{Receive Diversity}
\begin{itemize}
    \item Spatial diversity is typically achieved without sacrificing bandwidth, unlike frequency and time diversity which require more resources.
    \item Array gain refers to the power gain achieved by coherently combining the signals from multiple antennas compared to a single antenna case. It increases with the correlation of the spatial channel.
    \item Diversity gain is the improvement in signal-to-noise ratio (SNR) due to the independent fading paths in spatially diverse channels. It is maximized when the spatial channel is uncorrelated.
\end{itemize}

\subsection*{Beamforming}
The diagram also illustrates the concept of beamforming, where the antenna array is used to direct the energy of the transmitted signal in specific directions, enhancing the gain in the desired direction and reducing it in others, thus improving the overall link quality.


As wireless communication systems evolve, there is a trend towards higher carrier frequencies, which allows for the utilization of shorter wavelengths.

\subsubsection*{Uncorrelated Channels and Antenna Spacing}
For the channel to be considered uncorrelated between antenna elements, the spacing \( d_c \) is generally taken as half the wavelength \( \lambda \), i.e., \( d_c = \frac{\lambda}{2} \).

\subsubsection*{Advancements in Telecommunications}
\begin{itemize}
    \item The introduction of higher carrier frequencies leads to shorter wavelengths, enabling denser spatial diversity configurations.
    \item Technologies such as Wi-Fi have progressed from operating at 2.4 GHz (\( \lambda \approx 12.5 \) cm) to incorporating bands around 5 GHz (\( \lambda \approx 6 \) cm).
    \item The 5G networks are working at even higher frequencies like 3.8 GHz (\( \lambda \approx 8 \) cm) and planning for bands up to 52 GHz (\( \lambda \approx 6 \) mm), which allows for a significant increase in the number of antennas and thus potential array gain.
\end{itemize}

\subsection*{Wi-Fi 6 and Beyond}
Wi-Fi 6 (802.11ax) represents a significant step forward, offering enhancements such as:
\begin{itemize}
    \item Improved performance in dense environments.
    \item Higher throughput, claiming up to 40\% increase over previous standards.
    \item Greater network efficiency and extended battery life for connected devices.
\end{itemize}




\section*{SIMO Channel: Receive Diversity}

The receive diversity in a SIMO (Single Input, Multiple Output) channel utilizes multiple antennas at the receiver to improve the signal quality. The decision variable at the \(i\)-th receive antenna is given by:
\begin{equation}
    x_i(m) = h_i c_m + n_i(m),
\end{equation}
where \( h_i \) is the channel gain for the \(i\)-th antenna, \( c_m \) is the transmitted signal, and \( n_i(m) \) is the noise at the \(i\)-th antenna. The received signals \( x_i(m) \) are then combined to form a single decision statistic \( z(m) \), which is a weighted sum of the received signals. 

\subsection*{Combining the Signals}
The optimal combination of the signals can be expressed as:
\begin{equation}
    z(m) = w_1 x_1(m) + w_2 x_2(m) + \ldots + w_N x_N(m),
\end{equation}
where \( w_i \) are the weights chosen to maximize the SNR of the combined signal.

\subsection*{Handwritten Notes}
The handwritten notes from the image state that each channel is independently known, implying that the channel gains \( h_i \) are estimated separately for each receiving antenna. This allows the receiver to adaptively adjust the weights \( w_i \) for each antenna to maximize the overall SNR, which leads to the following optimization problem:
\begin{equation}
    \max_{\mathbf{w}} \frac{|\mathbf{w}^H \mathbf{h}|^2}{\mathbf{w}^H \mathbf{w}},
\end{equation}
where \( \mathbf{h} = [h_1, h_2, \ldots, h_N]^T \) and \( \mathbf{w} = [w_1, w_2, \ldots, w_N]^T \).

The notes mention that \( h_i \) is very large when \( w_i \) is very large, and vice versa, suggesting a proportional relationship between the weight and the channel gain for each antenna to achieve maximum SNR.

% Your figure environment and other LaTeX content goes here.



\section*{Maximal Ratio Combining (MRC) and Transmit Diversity in MISO Channels}

\subsection*{Maximal Ratio Combining (MRC)}
Maximal ratio combining utilizes multiple antenna elements to maximize the received signal strength. By applying the Schwarz inequality, we obtain an upper bound for the weighted sum of the channel gains:

\begin{equation}
\left( \sum_{i=1}^{N} w_i h_i \right)^2 \leq \sum_{i=1}^{N} w_i^2 \sum_{i=1}^{N} h_i^2.
\end{equation}

The optimal weighting coefficients, which maximize the SNR, are proportional to the respective channel gains:

\begin{equation}
w_i = h_i.
\end{equation}

This results in the signal-to-noise ratio (SNR) being:

\begin{equation}
\text{SNR} = \frac{A}{\sigma^2} \sum_{i=1}^{N} h_i^2.
\end{equation}

\subsection*{MISO Channel: Transmit Diversity}
In a MISO (Multiple Input Single Output) channel with \( M > 1 \) antennas at the transmitter and \( N = 1 \) at the receiver, spatial pre-coding is employed. The transmitted signal from the \( j \)-th antenna is given by:

\begin{equation}
y_j(m) = b_j^* c_m,
\end{equation}

where \( b_j \) is the precoding weight for the \( j \)-th transmit antenna, and \( c_m \) is the signal to be transmitted. The received signal is the sum of the signals from all transmit antennas:

\begin{equation}
x(m) = \sum_{j=1}^{M} h_j y_j(m).
\end{equation}





\section*{Maximal Ratio Transmit (MRT) Combining}

In Maximal Ratio Transmit (MRT) Combining, the precoding weight for the \(j\)-th transmit antenna is calculated as \( b_j = \frac{h_j}{\|h\|} \), where \( h \) is the channel vector and \( \|h\| \) is its norm. The transmitted signal at the receiver is a sum of the signals from each antenna weighted by this factor:

\begin{equation}
x(m) = \sum_{j=1}^{N} \frac{h_j}{\sqrt{N}} b_j c_m + n(m) = \sum_{j=1}^{N} \frac{|h_j|^2}{\|h\|} c_m + n(m),
\end{equation}

where \( c_m \) is the signal to be transmitted and \( n(m) \) is the noise. The Signal to Noise Ratio (SNR) is given by:

\begin{equation}
\text{SNR} = \frac{A}{\sigma^2} \sum_{i=1}^{N} |h_i|^2.
\end{equation}

\section*{Channel Gain \( \|h\|^2 \) Distribution for \( D \) Antennas}

For MRT or MRC, the SNR is proportional to the channel power gain. The probability density function (pdf) of \( \|h\|^2 \) depends on the number of antennas \( D \), and is given by:

\begin{equation}
f(\|h\|^2 = g) = \frac{1}{(D-1)!} g^{D-1} e^{-g}.
\end{equation}

\subsection*{Handwritten Notes}
The notes provide the calculation for the optimal weights in MRT and confirm the proportionality to the channel gains. The expectation calculations show that the energy of the signal is conserved after precoding, which is a crucial aspect of MRT. 

\begin{align*}
b_j &= \frac{h_j}{\|h\|}, \\
y_j &= b_j c, \\
\mathbb{E} \left[ \sum |b_j c_m|^2 \right] &= \sum |b_j|^2 A = \frac{\sum |h_j|^2}{\|h\|^2} A = A.
\end{align*}

% Your figure environment and other LaTeX content goes here.
\section*{Maximal Ratio Combining (MRC)}

Maximal Ratio Combining is a diversity technique used in wireless communications to combine multiple received signals into a single improved signal. The main benefits and drawbacks of MRC are:

\subsection*{Pros}
\begin{itemize}
\item Diversity gain: Improves signal quality by combining multiple signals.
\item Array gain: Increased signal strength due to multiple antennas.
\item MRT: No additional processing at the receiver is needed.
\end{itemize}

\subsection*{Cons}
\begin{itemize}
\item MRT: Requires channel knowledge at the transmitter.
\item MRC: Requires some extra processing at the receiver.
\end{itemize}

\section*{MIMO: Spatial Multiplexing}

MIMO technology employs spatial multiplexing to increase the capacity of a wireless channel. Spatial multiplexing is achieved through:

\begin{itemize}
\item The channel is represented as a \((N, M)\)-dimensional matrix.
\item The use of Singular Value Decomposition (SVD) of the channel matrix \( H \) to optimize transmission.
\item Coordinating the precoding weights at the transmitter with the combining weights at the receiver.
\end{itemize}

Assuming an equal number of transmit and receive antennas (\(M = N\)), spatial multiplexing can create \(N\) independent spatial channels, significantly increasing throughput.

\subsection*{Handwritten Notes}
The notes mention the advantages of diversity and array gains, emphasizing the lack of need for additional processing at the receiver in MRT and the requirement for some extra processing at the receiver in MRC. They also note the necessity for channel knowledge at the transmitter in MRT.

% Your figure environment and other LaTeX content goes here.
\section*{Singular Value Decomposition}

For any matrix \( A \in \mathbb{C}^{m \times n} \), it can be decomposed as \( A = U\Sigma V^H \) where:
\begin{itemize}
    \item \( U \in \mathbb{C}^{m \times p} \) and \( V \in \mathbb{C}^{n \times p} \) are unitary matrices.
    \item \( \Sigma \in \mathbb{C}^{p \times p} \) is a diagonal matrix with non-negative real numbers on the diagonal.
    \item \( p = \min(m,n) \) represents the number of singular values of \( A \).
\end{itemize}

In MIMO systems, this decomposition helps to transform the channel into parallel SISO channels, facilitating the design of spatial multiplexing schemes.

\subsection*{Optimal MIMO Scheme: Spatial Multiplexing}
By pre-multiplying the signal vector by \( V \) and post-multiplying the received signal by \( U^H \), we can decouple the MIMO channel into \( p \) independent channels, where \( p \) is the rank of the channel matrix \( H \), and \( H = U\Sigma V^H \).

\subsection*{Handwritten Notes Transcription}
The handwritten notes confirm the properties of the matrices \( U \) and \( V \) being unitary, and \( \Sigma \) being diagonal. They also explain that the received vector \( y \) in a MIMO system can be expressed as \( y = Hx + n \), and after processing with \( U^H \), it simplifies to \( z = \Sigma s + n' \) which represents parallel SISO channels. If \( H \) is square and full rank, the optimal weights are given by the eigenvectors, and the signal can be recovered with maximal efficiency.
\section*{Adaptive Modulation and Coding}

The Shannon capacity formula indicates the maximum rate achievable over a transmission channel, which is given by:
\begin{equation}
    C = B\log_2(1 + \text{SNR})
\end{equation}
where the spectral efficiency is measured in b/s/Hz, \( B \) is the bandwidth, and SNR (Signal-to-Noise Ratio) is defined for channel gain \( H \) as:
\begin{equation}
    \text{SNR} = \frac{|H|^2P}{\sigma^2}
\end{equation}

In a practical system, spectral efficiency depends on the modulation order and the coding rate. Given the symbol timing \( T \) and symbol rate \( R_s = \frac{1}{T} \approx B \), for a modulation order \( M \) and a coding rate \( R \), the bit rate \( R_b \) is expressed as:
\begin{equation}
    R_b = \log_2 M \frac{R}{T} \approx mRB
\end{equation}

As \( R_b < C \), the modulation order and the coding rate are bounded by:
\begin{equation}
    mR < \log_2(1 + \text{SNR})
\end{equation}

\subsection*{Handwritten Notes}
The notes detail the process of transmission, highlighting the mapping of information bits through coding to symbols. They express the relationship between the coded bits \( m \times \text{symbol} \) and information bits \( m \times R \), resulting in a formula for calculating \( K \), the number of information bits:
\begin{equation}
    K = R_m \cdot m = \frac{k}{n} m
\end{equation}






\section*{Adaptive Modulation and Coding}

Adaptive modulation and coding (AMC) is a method used in wireless communication systems where the modulation format and coding rate are dynamically adapted to the prevailing channel conditions. This technique aims to maximize spectral efficiency and link reliability. In practice, AMC is implemented by directly mapping the Channel Quality Indicator (CQI) to a specific Modulation and Coding Scheme (MCS).

\begin{table}[htbp]
\centering
\caption{Adaptive Modulation and Coding Schemes}
\begin{tabular}{|c|c|c|c|c|}
\hline
Radio Bearer Index & Name & Modulation & Channel Coding Rate & Bearer Efficiency (bits/symbol) \\
\hline
1 & QPSK \(1/12\) & QPSK & 0.0761719 & 0.1523 \\
2 & QPSK \(1/9\) & QPSK & \ldots & \ldots \\
\ldots & \ldots & \ldots & \ldots & \ldots \\
15 & 64QAM \(11/12\) & 64QAM & 0.925781 & 5.5547 \\
\hline
\end{tabular}
\end{table}

Multicarrier transmissions over selective channels experience a set of parallel channels with diverse channel gains. This frequency diversity can be exploited by adapting the modulation format and the coding rate to the quality of the channel.

The bearer efficiency for each MCS is calculated by:
\begin{equation}
\text{Efficiency} = \log_2(M) \cdot \text{Coding Rate}
\end{equation}

Where \( M \) is the modulation order, and the coding rate is a fraction representing the number of useful bits out of the total bits transmitted. The higher the modulation order and the coding rate, the greater the spectral efficiency, given by:
\begin{equation}
\text{Spectral Efficiency} = B \cdot \text{Efficiency}
\end{equation}

However, the modulation order and the coding rate are constrained by the SNR to ensure reliable transmission.





\section*{Optimal Power Distribution}

The optimal power distribution in a multi-channel system can be described by the water-filling algorithm. The objective is to maximize the total rate over $N$ channels under a total power constraint $P_0$. Each channel has a corresponding noise level $\sigma_n^2$, and the water-filling algorithm is utilized to allocate power across these channels efficiently.

\subsection*{Formulation}
The problem can be formulated as:
\begin{equation}
\begin{aligned}
& \underset{P}{\text{maximize}}
& & \sum_{n=1}^{N} \log_2 \left(1 + \frac{P_n}{\sigma_n^2}\right) \\
& \text{subject to}
& & P_n \geq 0, \; n = 1, \ldots, N, \\
&&& \sum_{n=1}^{N} P_n = P_0,
\end{aligned}
\end{equation}
where $\sigma_n^2 = \frac{\sigma^2}{|H_n|^2}$ and $H_n$ is the channel gain.

\subsection*{Solution using Lagrange Multipliers}
The solution to the power allocation problem utilizes Lagrange multipliers:
\begin{equation}
P_n^* = \left(\mu - \frac{\sigma_n^2}{|H_n|^2}\right)^+
\end{equation}
where $\mu$ is the water level determined by the power constraint and $(x)^+ = \max(x, 0)$.

The Lagrangian is given by:
\begin{equation}
\mathcal{L}(P, \lambda) = \sum_{n=1}^{N} \log_2 \left(1 + \frac{P_n}{\sigma_n^2}\right) + \lambda \left(P_0 - \sum_{n=1}^{N} P_n\right)
\end{equation}

And the Karush-Kuhn-Tucker (KKT) conditions provide the optimal power levels for each channel.

\subsection*{Handwritten Notes Clarification}
From the handwritten notes, we have additional clarifications on the water-filling algorithm and its implications:
\begin{itemize}
    \item The channel SNR is a factor in determining the distribution of power.
    \item When a channel's power is zero, it is excluded from the rate calculation, reflecting the water-filling analogy.
    \item The KKT conditions lead to the equation for the optimal power allocation, considering the noise level and the channel gain.
\end{itemize}





\section*{Waterfilling: Computing the Water Level \(\mu\)}

The water level \(\mu\) in the water-filling algorithm is critical as it determines the power allocation across the channels. It is calculated by solving the equation that ensures the total power used equals the power constraint \(P_0\).

\subsection*{The Nonlinear Equation}
The value of \(\mu\) satisfies the equation:
\begin{equation}
\sum_{n=1}^{N} (\mu - \sigma_n^2)^+ = P_0,
\end{equation}
where \(\sigma_n^2\) is the noise power in the \(n\)-th channel and \((x)^+\) is the positive part of \(x\).

\subsection*{Bisection Method for Solving for \(\mu\)}
To find \(\mu\), we apply the bisection method which is an iterative procedure to find roots of continuous functions.

\subsubsection*{The Function}
We define the function for the bisection method:
\begin{equation}
f(\mu) = \sum_{n=1}^{N} (\mu - \sigma_n^2)^+ - P_0,
\end{equation}
which is a nonlinear function of \(\mu\).

\subsubsection*{The Iterative Process}
The bisection method starts with two initial points \(a_1\) and \(b_1\) such that \(f(a_1) \leq 0\) and \(f(b_1) \geq 0\). It then proceeds iteratively to narrow down the interval containing the root by evaluating the function at the midpoint and selecting the subinterval where a sign change occurs.

\subsubsection*{The Algorithm}
\begin{align*}
&\text{while } |b - a| > \text{tolerance} \\
&\quad c = \frac{a + b}{2} \\
&\quad \text{if } f(c) = 0 \text{ or } (b - a)/2 < \text{tol} \text{ then} \\
&\quad \quad \text{root} = c \\
&\quad \text{end if} \\
&\quad \text{if } \text{sign}(f(c)) = \text{sign}(f(a)) \text{ then} \\
&\quad \quad a = c \\
&\quad \text{else} \\
&\quad \quad b = c \\
&\quad \text{end if} \\
&\text{end while}
\end{align*}

The iteration continues until the interval is sufficiently small, and the midpoint \(c\) is an approximation to the root.


\section*{The Bisection Method for Waterfilling Solution}

The bisection method is an efficient algorithm to find the root of a function, which in the context of the waterfilling algorithm is used to compute the water level \(\mu\). 

The main principle is to iteratively reduce the search interval by half and select the subinterval where the sign of \(f\) changes.

\subsection*{Bisection Algorithm Steps}
Let's denote \(a_k\) and \(b_k\) as the lower and upper bounds of the interval at the \(k\)-th iteration, and \(f\) as our function of interest. The algorithm proceeds as follows:

\begin{itemize}
\item Compute the midpoint \(c_k = \frac{a_k + b_k}{2}\).
\item Evaluate the function at the midpoint: \(f(c_k)\).
\item If \(f(c_k)\) is very small or zero, \(c_k\) is the root and the algorithm stops.
\item If \(f(c_k)\) has the same sign as \(f(a_k)\), then set \(a_{k+1} = c_k\) and \(b_{k+1} = b_k\).
\item Otherwise, set \(a_{k+1} = a_k\) and \(b_{k+1} = c_k\).
\end{itemize}

The algorithm stops when the interval \( |b_k - a_k| \) is sufficiently small.

The figure represents the waterfilling method applied to an OFDM channel, where each channel gain can be visualized as a 'depth', and the water level \(\mu\) is set to equalize the total power distributed across the channels.

\section*{Waterfilling Example: Optimal Power Allocation}

The waterfilling algorithm is a strategy used in communication systems to distribute power across various channels to maximize the total communication rate.

\subsection*{Algorithm Overview}
The waterfilling algorithm allocates more power to channels with better channel-to-noise ratios, akin to 'filling' the channels with water up to a certain 'water level' \(\mu\).

\subsection*{Mathematical Formulation}
The power allocated to the \(n\)-th channel \(P_n\) is determined by:

\[
P_n = \left( \mu - \sigma_{n}^{2} \right)^{+}
\]

where \(\sigma_{n}^{2}\) represents the normalized noise in the \(n\)-th channel, and \((x)^{+}\) denotes the positive part of \(x\). The water level \(\mu\) is found such that the total power constraint is satisfied:

\[
\sum_{n=1}^{N} \left( \mu - \sigma_{n}^{2} \right)^{+} = P_0
\]

This ensures that the power allocated does not exceed the total available power \(P_0\).

In the figure, the blue bars represent the power allocated to each channel, while the red line indicates the channel normalized noise. The power is allocated more to the channels where the blue bar exceeds the red line, i.e., the channel-to-noise ratio is higher.

\subsection*{Achieved Rate vs Power}
The graph below illustrates the achieved communication rate as a function of power. The waterfilling strategy outperforms uniform power allocation strategies, especially on channels with better conditions, leading to an increased average rate per unit of bandwidth.

% Include any additional graphs or figures here.